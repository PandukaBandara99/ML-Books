{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZmhOh6FMQedsiJzs9E+91",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PandukaBandara99/ML-Books/blob/main/MLOPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mahela Panduka Bandara ; <br>\n",
        "UG @ Dpt. of Electronic & Electrical Engineering ;<br>\n",
        "University of Peradeniya ;<br>\n",
        "2023/05<br>\n",
        "\n",
        "Email : e19039@end.pdn.ac.lk<br>\n",
        "LinkedIN : https://www.linkedin.com/in/pandukabandara/<br>\n",
        "\n",
        "References :"
      ],
      "metadata": {
        "id": "FSQCIosDFefp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.0 Introduction to MLOps"
      ],
      "metadata": {
        "id": "gnN764ypFfP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLOps, short for \"Machine Learning Operations,\" is a set of practices and principles that aims to streamline the development, deployment, monitoring, and maintenance of machine learning (ML) models in production environments. It borrows concepts from DevOps (Development Operations) and applies them specifically to the machine learning lifecycle. MLOps seeks to bridge the gap between data scientists, ML engineers, and operations teams to foster collaboration and ensure the successful integration of ML models into real-world applications.\n",
        "\n",
        "\n",
        "1. **Reproducibility:** Ensuring that ML models can be reproduced consistently across different environments, enabling consistent results from development to production.\n",
        "\n",
        "2. **Automation:** Automating the various stages of the ML workflow, such as data preprocessing, feature engineering, model training, and deployment. This reduces human errors, saves time, and increases efficiency.\n",
        "\n",
        "3. **Versioning and Tracking:** Implementing version control for ML artifacts, including data, models, and code. This enables the team to track changes, revert to previous versions, and collaborate effectively.\n",
        "\n",
        "4. **Continuous Integration and Continuous Deployment (CI/CD):** Applying the principles of CI/CD to ML, allowing for seamless and automated integration of new models into production systems as they are developed and tested.\n",
        "\n",
        "5. **Monitoring and Logging:** Setting up monitoring systems to track model performance, data drift, and other metrics in real-time to identify and address issues promptly.\n",
        "\n",
        "6. **Scalability and Resource Management:** Ensuring that ML models can handle the demands of production environments efficiently, including managing computing resources effectively.\n",
        "\n",
        "7. **Security and Governance:** Addressing data privacy concerns and ensuring that models comply with regulatory requirements."
      ],
      "metadata": {
        "id": "eRG4Wj2FFkc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Machine Learning Project Lifecycle"
      ],
      "metadata": {
        "id": "L7y7qKW4FuRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Scoping:**\n",
        "   - In this initial stage, the project's objectives and requirements are defined in collaboration with stakeholders and domain experts.\n",
        "   - The team identifies the problem to be solved, the business goals, and the metrics to evaluate the model's performance.\n",
        "   - It is essential to set clear expectations and understand the limitations and potential challenges of the project.\n",
        "\n",
        "2. **Data Collection:**\n",
        "   - Data collection involves gathering the relevant datasets needed to train and evaluate the machine learning model.\n",
        "   - The data should be representative of the real-world scenario to ensure the model's effectiveness in production.\n",
        "   - Data preprocessing steps, such as cleaning, handling missing values, and feature engineering, are often performed in this stage.\n",
        "\n",
        "3. **Modeling:**\n",
        "   - In the modeling stage, data scientists and ML engineers select appropriate algorithms and techniques to build the machine learning model.\n",
        "   - The data is split into training, validation, and test sets to train the model, tune hyperparameters, and evaluate its performance.\n",
        "   - Model selection and evaluation are iterative processes, and various techniques like cross-validation are used to ensure generalization to unseen data.\n",
        "\n",
        "4. **Deployment:**\n",
        "   - Once a satisfactory model is developed, it needs to be deployed in a production environment to serve real-world requests.\n",
        "   - Deployment may involve creating APIs, integrating the model with existing systems, or deploying it as a service.\n",
        "   - Monitoring and logging mechanisms are put in place to track the model's performance, detect anomalies, and address potential issues.\n"
      ],
      "metadata": {
        "id": "0N_pluO9GACg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Deployment Criteria"
      ],
      "metadata": {
        "id": "cLNPMMHjPKlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Real-time or Batch:**\n",
        "   - Determine whether the use case requires real-time predictions or batch predictions.\n",
        "   - Real-time predictions are suitable for applications that need immediate responses, while batch predictions are suitable for processing large volumes of data at once.\n",
        "\n",
        "2. **Cloud vs. Edge/Browser:**\n",
        "   - Choose between hosting the prediction service in the cloud or deploying it on edge devices or within the user's browser.\n",
        "   - Cloud-based solutions offer scalability and central management, while edge/browser deployments provide lower latency and reduced reliance on external infrastructure.\n",
        "\n",
        "3. **Compute Resources (CPU, GPU, Memory):**\n",
        "   - Evaluate the computational requirements of the deployed machine learning models.\n",
        "   - Decide whether CPU-based servers are sufficient or if GPU acceleration is needed for complex models. Also, consider the memory requirements for handling concurrent requests.\n",
        "\n",
        "4. **Latency and Throughput:**\n",
        "   - Assess the desired response time (latency) for predictions and the number of predictions the server needs to handle concurrently (throughput).\n",
        "   - Optimize the prediction server's configuration to meet the latency and throughput requirements.\n",
        "\n",
        "5. **Logging, Security, and Privacy:**\n",
        "   - Implement robust logging to monitor server performance, user activity, and errors.\n",
        "   - Prioritize security by employing authentication and authorization mechanisms to protect data and models from unauthorized access.\n",
        "   - Ensure compliance with privacy regulations and handle sensitive data appropriately.\n",
        "\n",
        "6. **Concept Drift and Data Drift:**\n",
        "   - Consider how the prediction server will handle concept drift (changes in the underlying data distribution) and data drift (changes in the input data).\n",
        "   - Implement mechanisms to periodically retrain models or incorporate drift detection and adaptation strategies.\n"
      ],
      "metadata": {
        "id": "sB0wZ-sNPHF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Deployment Patterns"
      ],
      "metadata": {
        "id": "ug9VjfPQwImW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Shadow Deployment:**\n",
        "   In a shadow deployment, a new version of a machine learning model is deployed alongside the existing production model. It receives real-world data but its predictions are not used for any critical decisions. This allows developers to monitor and compare the performance of the new model against the existing one before fully transitioning.\n",
        "\n",
        "2. **Canary Deployment:**\n",
        "   Canary deployment involves releasing a new ML model or application version to a subset of users or instances, while the majority of users still interact with the old version. This allows for gradual testing and monitoring of the new model's performance in real-world scenarios before rolling it out to everyone. It acts as an early warning system, alerting to potential issues.\n",
        "\n",
        "3. **Blue-Green Deployment:**\n",
        "   In blue-green deployment, two separate environments (blue and green) are maintained, with only one actively serving traffic at a time. The new version is deployed in the \"green\" environment while the existing version continues to operate in the \"blue\" environment. Once the green environment is thoroughly tested and validated, traffic is switched from blue to green, making the new version live.\n"
      ],
      "metadata": {
        "id": "mPSUSXr8wMpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 BaseLine for Performance Improvement"
      ],
      "metadata": {
        "id": "sPDYYPNK8vHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline helps to indicate what meight be possible. In some cases (Such as HLP) is also gives a sense of what is irreducible error/Bayes error.\n",
        "\n",
        "Baseline can be determined by\n",
        "- Comparing with human level performance\n",
        "- Literature survay\n",
        "- Quick and dirty implimentation\n",
        "- Performance of older system"
      ],
      "metadata": {
        "id": "7PYkFS11820w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Confusion Matrix"
      ],
      "metadata": {
        "id": "N66CNaSn05Oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a fundamental tool in the field of machine learning, especially for evaluating the performance of classification algorithms. It provides a clear representation of how well a classifier is performing by showing the counts of various types of predictions compared to the actual ground truth.\n",
        "\n",
        "In a binary classification scenario (where there are two classes: positive and negative), a confusion matrix is typically organized as follows:\n",
        "\n",
        "```\n",
        "                       Actual Positive   Actual Negative\n",
        "Predicted Positive     True Positive     False Positive\n",
        "Predicted Negative     False Negative    True Negative\n",
        "```\n",
        "\n",
        "\n",
        "- **True Positive (TP)**: The classifier correctly predicted a positive instance.\n",
        "- **True Negative (TN)**: The classifier correctly predicted a negative instance.\n",
        "- **False Positive (FP)**: The classifier incorrectly predicted a positive instance when the actual class is negative (Type I error).\n",
        "- **False Negative (FN)**: The classifier incorrectly predicted a negative instance when the actual class is positive (Type II error).\n",
        "\n",
        "With these values, you can calculate various performance metrics to assess the classifier's effectiveness, such as:\n",
        "<br>\n",
        "\n",
        "1. **Accuracy**: <br>\n",
        "  $$\\frac{True \\ predictions}{All\\ predictions} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
        "   It measures the proportion of correctly predicted instances among all instances. <br>\n",
        "\n",
        "\n",
        "2. **Precision**: <br>\n",
        "\n",
        "    $$\\frac{TP}{TP+FP}$$\n",
        "   It indicates the proportion of correctly predicted positive instances out of all instances predicted as positive. Precision focuses on the accuracy of positive predictions.\n",
        "\n",
        "3. **Recall (Sensitivity or True Positive Rate)**:\n",
        "\n",
        "  $$\\frac{TP}{TP+FN}$$\n",
        "   It measures the proportion of correctly predicted positive instances out of all actual positive instances. Recall focuses on the ability to capture positive instances.\n",
        "\n",
        "4. **Specificity (True Negative Rate)**:\n",
        "    $$\\frac{TN}{TN + FP}$$\n",
        "   It measures the proportion of correctly predicted negative instances out of all actual negative instances.\n",
        "\n",
        "5. **F1-Score**:\n",
        "\n",
        "    $$ 2\\times \\frac{Precision\\ \\times\\ Recall}{Precision+Recall}$$\n",
        "   It is the harmonic mean of precision and recall, providing a balanced measure between the two.\n",
        "\n",
        "Confusion matrices become even more valuable in multi-class classification problems, where the matrix is extended to handle more than two classes. Each entry in the matrix represents the count of instances for a particular predicted class against the actual class.\n"
      ],
      "metadata": {
        "id": "nedDGdY4084m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example**"
      ],
      "metadata": {
        "id": "1i5Ffaew3BRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume you are working with an email service provider, and they want to improve their spam email filtering system. They provide you with a dataset containing 200 emails, along with their true labels. Your task is to train a machine learning model to classify new emails as either \"Spam\" or \"Not Spam\" based on the content of the email.\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "Out of the 200 emails, 150 are \"Not Spam\" and 50 are \"Spam\". Train the model on this dataset and then evaluate its performance using a test dataset.\n",
        "\n",
        "**Confusion Matrix:**\n",
        "\n",
        "Using the test dataset, let's say the model makes the following predictions:\n",
        "\n",
        "- True Positive (TP): 40 emails\n",
        "- True Negative (TN): 100 emails\n",
        "- False Positive (FP): 10 emails\n",
        "- False Negative (FN): 5 emails\n",
        "\n",
        "The confusion matrix can be represented:\n",
        "\n",
        "$$\n",
        "\\begin{matrix}\n",
        "& \\text{Actual Not Spam} & \\text{Actual Spam} \\\\\n",
        "\\text{Predicted Not Spam} & \\text{TN} = 100 & \\text{FP} = 10 \\\\\n",
        "\\text{Predicted Spam} & \\text{FN} = 5 & \\text{TP} = 40 \\\\\n",
        "\\end{matrix}\n",
        "$$\n",
        "\n",
        "**Metrics:**\n",
        "\n",
        "1. **Accuracy**: $\\frac{TP + TN}{TP + TN + FP + FN} = \\frac{40 + 100}{40 + 100 + 10 + 5} = 0.9$ or 90%\n",
        "\n",
        "2. **Precision**: $\\frac{TP}{TP + FP} = \\frac{40}{40 + 10} = 0.8$ or 80%\n",
        "\n",
        "3. **Recall (Sensitivity)**: $\\frac{TP}{TP + FN} = \\frac{40}{40 + 5} = 0.8889$ or 88.89%\n",
        "\n",
        "4. **Specificity**: $\\frac{TN}{TN + FP} = \\frac{100}{100 + 10} = 0.9091$ or 90.91%\n",
        "\n",
        "5. **F1-Score**: $2 \\times \\frac{Precision \\times Recall}{Precision + Recall} = 2 \\times \\frac{0.8 \\times 0.8889}{0.8 + 0.8889} = 0.8421$ or 84.21%\n",
        "\n",
        "**Some conclusions about the model:**\n",
        "\n",
        "- The model's accuracy is high (90%), indicating that it's performing well overall.\n",
        "- Precision (80%) tells us that when the model predicts an email as \"Spam,\" it's correct 80% of the time.\n",
        "- Recall (88.89%) indicates that the model is able to correctly identify 88.89% of the actual \"Spam\" emails.\n",
        "- Specificity (90.91%) indicates the model's ability to correctly identify \"Not Spam\" emails.\n"
      ],
      "metadata": {
        "id": "VANZPNd_3Dud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{matrix}\n",
        "& \\text{Actual Not Disese} & \\text{Actual Disese} \\\\\n",
        "\\text{Predicted Not Disese} & \\text{TN} = 0 & \\text{FP} = 0 \\\\\n",
        "\\text{Predicted Disese} & \\text{FN} = 98 & \\text{TP} = 2 \\\\\n",
        "\\end{matrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "BcexL35RH22O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Data Consitancy"
      ],
      "metadata": {
        "id": "IPHCRIpxRXpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6.1 Label Consistency"
      ],
      "metadata": {
        "id": "G5v50dgGTLRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To address label consistency issues, several steps can be taken:\n",
        "\n",
        "1. **Annotation Quality Control:** Implement strict quality control measures during the data labeling process. Provide clear labeling guidelines and encourage annotators to seek clarification when uncertain.\n",
        "\n",
        "2. **Multiple Annotations:** For critical tasks, having multiple annotators label the same data can help identify and rectify discrepancies. This approach can provide a consensus or highlight cases requiring further review.\n",
        "\n",
        "3. **Error Analysis:** Conduct a systematic review of labeled instances to identify patterns of inconsistency. This analysis can help refine labeling guidelines and improve annotation accuracy.\n",
        "\n",
        "4. **Feedback Loop:** Establish a feedback loop between annotators and model developers to address labeling challenges, clarify ambiguities, and continuously improve label quality.\n",
        "\n",
        "5. **Regular Audits:** Periodically audit a subset of labeled data to assess ongoing label consistency. This process can help identify issues that might arise over time.\n",
        "\n",
        "6. **Active Learning:** Incorporate active learning techniques to iteratively select instances that the model is uncertain about. This approach can help focus annotation efforts on challenging cases, improving both label quality and model performance."
      ],
      "metadata": {
        "id": "eI4251Q7TRVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Small Data:**\n",
        "\n",
        "1. **Volume:** Datasets with a limited number of instances.\n",
        "2. **Labelers:** Involves a small number of labelers due to the manageable data size.\n",
        "3. **Labeling Process:** Labelers can engage in direct discussions about specific labels, aiding in clarifying ambiguous cases.\n",
        "4. **Consistency:** Achieving label consistency is more feasible within a small group, as discussions can lead to a shared understanding.\n",
        "5. **Labeling Instructions:** Specific and personalized instructions can be sent to labelers, allowing for detailed guidance.\n",
        "6. **Labeler Agreement:** Multiple labelers can annotate each example, and voting or consensus labels can be used to enhance accuracy.\n",
        "7. **Resource Advantage:** Easier to manage, control, and validate labeling quality due to the smaller dataset.\n",
        "\n",
        "**Big Data:**\n",
        "\n",
        "1. **Volume:** Big data involves massive datasets with a substantial number of instances.\n",
        "2. **Labelers:** Due to the scale, many labelers might be needed to handle the data volume effectively.\n",
        "3. **Labeling Process:** Direct discussions about individual labels may not be feasible due to the sheer number of instances.\n",
        "4. **Consistency: **Ensuring label consistency becomes more challenging as the number of labelers and instances increases.\n",
        "5. **Labeling Instructions:** Generalized instructions might be necessary to handle the large number of labelers.\n",
        "6. **Labeler Agreement:** Achieving consensus from multiple labelers for each example could be resource-intensive.\n",
        "7. **Resource Challenge:** Requires significant resources for managing, validating, and coordinating labeling efforts."
      ],
      "metadata": {
        "id": "Beorwb6rTj14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6.2 HLP - Human Level Perfomance"
      ],
      "metadata": {
        "id": "QqQK8WVETUVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The point at which an AI model's performance matches or surpasses the capabilities of human experts in a specific task. Achieving HLP is a significant milestone as it signifies the model's effectiveness and potential in real-world applications.\n",
        "\n",
        "1. **Methods of Measurement:**\n",
        "To measure HLP, various metrics are used, such as accuracy, precision, recall, and F1-score, depending on the nature of the task. The comparison is often made between the model's predictions and human annotations.\n",
        "\n",
        "2. **Ground Truth Label:**\n",
        "The ground truth label represents the correct answer or annotation for a given data point. It serves as the reference against which model predictions are evaluated. In many cases, human experts generate ground truth labels.\n",
        "\n",
        "3. **Level of Agreement:**\n",
        "Level of agreement refers to the consistency among human labelers when annotating data. It is often measured using inter-rater agreement metrics such as Cohen's Kappa or Fleiss' Kappa. These metrics quantify the extent to which labelers agree beyond what would be expected by chance.\n",
        "\n",
        "4. **Bayes Error:**\n",
        "Bayes error, also known as the irreducible error, represents the lowest possible error rate that any model can achieve on a task. It is the error rate when the model makes predictions based on the Bayes optimal decision boundary, which requires knowing the underlying data distribution.\n",
        "\n",
        "The formula for Bayes error is:\n",
        "$$ \\text{Bayes Error} = \\int \\min(P(y|x)) \\cdot P(x) \\, dx $$\n",
        "where $ P(y|x) $ is the conditional distribution of the correct label given the input data $ x $, and $ P(x) $ is the distribution of the input data. <br> <br>\n",
        "\n",
        "**Example:**\n",
        "Consider a binary classification task where an AI model predicts whether an email is spam $ (y = 1) $ or not $( y = 0 )$ based on certain features $ x $. The ground truth labels indicate that out of 100 emails, 80 are correctly labeled.\n",
        "\n",
        "- Accuracy of the AI model: $ \\text{Accuracy} = \\frac{\\text{Correctly Classified}}{\\text{Total}} = \\frac{80}{100} = 0.8 $\n",
        "\n",
        "- Bayes error is the lowest achievable error rate. Let's say the optimal distribution of spam and non-spam emails in the data is $P(y=1) = 0.6 $ and $ P(y=0) = 0.4 $, then the Bayes error can be calculated as:\n",
        "$ \\text{Bayes Error} = \\min(0.6, 0.4) = 0.4 $\n",
        "\n"
      ],
      "metadata": {
        "id": "oAXXOJvAUf52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6.3 Increasing the Human Level Perfomance"
      ],
      "metadata": {
        "id": "6egSekux4G54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **High-Quality Training Data:**\n",
        "   Acquire and curate a diverse and representative dataset for training. High-quality data ensures that the model learns from accurate and reliable examples, helping it generalize better to real-world scenarios.more consistent labels will raise ML performance, which is ultimately likely to benefit the actual application performance.\n",
        "\n",
        "\n",
        "2. **Feature Engineering and Selection:**\n",
        "   Devote time to thoughtful feature engineering. Identify relevant features and discard irrelevant or redundant ones. Human experts often leverage domain knowledge to extract meaningful information from data, enhancing the model's performance.\n",
        "\n",
        "3. **Incorporate Expert Feedback:**\n",
        "   Continuously involve human experts in the training process. Regular feedback and iterations help refine the model's predictions, aligning them with human-level expertise.\n",
        "\n",
        "4. **Ensemble Methods and Model Diversity:**\n",
        "   Utilize ensemble techniques that combine predictions from multiple models. Each model could excel in different aspects, mitigating biases and improving overall performance. This approach can help reach or surpass human-level performance.\n",
        "\n",
        "5. **Active Learning and Human-in-the-Loop:**\n",
        "   Implement active learning strategies where the model identifies instances it's uncertain about and seeks human input for labeling. This iterative process allows the model to focus on challenging cases, gradually improving its capabilities.\n"
      ],
      "metadata": {
        "id": "tsifLHmV4Lcu"
      }
    }
  ]
}