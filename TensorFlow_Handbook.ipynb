{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PandukaBandara99/ML-Books/blob/main/TensorFlow_Handbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mahela Panduka Bandara ; <br> UG @ Dpt. of Electronic & Electrical Engineering ;<br> University of Peradeniya ;<br> 2023/05 <br><br> Email : e19039@end.pdn.ac.lk <br> LinkedIN : https://www.linkedin.com/in/pandukabandara/"
      ],
      "metadata": {
        "id": "2erwzCdp065W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References : \n",
        "1. https://www.coursera.org/professional-certificates/tensorflow-in-practice"
      ],
      "metadata": {
        "id": "GpW5eGWJ098J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x # only for colab\n",
        "import tensorflow as tf\n",
        "print(tf.version)"
      ],
      "metadata": {
        "id": "rpoPvfwjzrwQ",
        "outputId": "17440557-d92c-48f6-8279-1203a027483b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DEl\n"
      ],
      "metadata": {
        "id": "pbDoIzWE1D7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, here is an example table of contents for a book named \"TensorFlow Handbook\":\n",
        "\n",
        "1. Introduction to TensorFlow\n",
        "   - What is TensorFlow?\n",
        "   - TensorFlow vs other machine learning frameworks\n",
        "   - Getting started with TensorFlow\n",
        "2. TensorFlow Basics\n",
        "   - Tensors and operations\n",
        "   - Variables and placeholders\n",
        "   - Graphs and sessions\n",
        "   - Saving and restoring models\n",
        "3. Building Neural Networks with TensorFlow\n",
        "   - Neural network architecture\n",
        "   - Activation functions\n",
        "   - Loss functions\n",
        "   - Optimizers\n",
        "4. Convolutional Neural Networks\n",
        "   - Convolutional layers\n",
        "   - Pooling layers\n",
        "   - Building a convolutional neural network\n",
        "5. Recurrent Neural Networks\n",
        "   - Long Short-Term Memory (LSTM) networks\n",
        "   - Building a recurrent neural network\n",
        "6. Reinforcement Learning with TensorFlow\n",
        "   - Q-Learning and SARSA\n",
        "   - Building a reinforcement learning agent\n",
        "7. Advanced TensorFlow Topics\n",
        "   - Transfer learning\n",
        "   - Distributed TensorFlow\n",
        "   - TensorBoard\n",
        "8. Applications of TensorFlow\n",
        "   - Computer vision\n",
        "   - Natural language processing\n",
        "   - Time series analysis\n",
        "9. Future of TensorFlow\n",
        "   - Current research and developments\n",
        "   - Challenges and future directions\n",
        "10. Conclusion\n",
        "   - Recap of key concepts\n",
        "   - Resources for further learning."
      ],
      "metadata": {
        "id": "RpFyxUCk2CvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction to TensorFlow"
      ],
      "metadata": {
        "id": "ZxIVQ42u2Gl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Introduction"
      ],
      "metadata": {
        "id": "tmxwQJBx2OEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow is an open-source library for data flow and differentiable programming that allows developers to build and deploy machine learning models for a variety of tasks. Developed by the Google Brain team in 2015, TensorFlow has quickly become one of the most popular machine learning libraries due to its ease of use, flexibility, and scalability.\n",
        "\n",
        "Compared to other machine learning libraries such as PyTorch, Keras, and Scikit-Learn, TensorFlow has some unique features that make it a top choice for machine learning tasks. The following table compares TensorFlow with other popular libraries:\n",
        "\n",
        "| Library     | Strengths                                                                                  | Weaknesses                                                                               |\n",
        "|-------------|--------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
        "| TensorFlow  | High scalability and performance, large community and support, production-ready features | Steep learning curve, can be overly complex for simple tasks, slower development process |\n",
        "| PyTorch     | Dynamic computational graphs, intuitive API, good for research and experimentation        | Limited scalability, smaller community compared to TensorFlow, slower training speed     |\n",
        "| Keras       | User-friendly API, easy to learn and use, good for rapid prototyping                        | Limited flexibility and scalability, less production-ready features than TensorFlow      |\n",
        "| Scikit-Learn | Simple and easy to use API, good for classical machine learning tasks                       | Limited support for deep learning and neural networks, less scalable than TensorFlow     |\n"
      ],
      "metadata": {
        "id": "Ns18fPsB2o4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Neural networks"
      ],
      "metadata": {
        "id": "YfPWefig2lo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks are a class of machine learning algorithms that are inspired by the structure and function of the human brain. They are composed of interconnected nodes, called neurons, that process and transmit information throughout the network. Neural networks have shown remarkable success in a variety of applications, including image and speech recognition, natural language processing, and autonomous driving.\n",
        "\n",
        "The basic building block of a neural network is the neuron, which receives input from other neurons or from external data sources. Each input is weighted according to its importance, and the neuron applies an activation function to the sum of the weighted inputs to produce an output. This output is then passed to other neurons in the network as input.\n",
        "\n",
        "Activation functions are a crucial component of neural networks, as they introduce nonlinearity and allow the network to model complex relationships between inputs and outputs. There are several common activation functions used in neural networks, including:\n",
        "\n",
        "> 1. **Sigmoid:** The sigmoid function is a popular choice for binary classification problems, as it maps any input to a value between 0 and 1. Its equation is: \n",
        "> ```css\n",
        "> f(x) = 1 / (1 + e^-x)\n",
        "> ```\n",
        "> 2. **ReLU:** The rectified linear unit (ReLU) function is widely used in deep learning networks. It applies a threshold at zero, producing a linear output for positive inputs and zero for negative inputs. Its equation is:\n",
        "> ```css\n",
        "> f(x) = max(0, x)\n",
        "> ```\n",
        "> 3. *Tanh:* The hyperbolic tangent (tanh) function is similar to the sigmoid function, but it maps inputs to values between -1 and 1. Its equation is:\n",
        "> ```css\n",
        "> f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
        "> ```\n",
        "> 4. **Softmax:** The softmax function is often used in multi-class classification problems, as it produces a probability distribution over multiple output classes. Its equation is:\n",
        "> ```css\n",
        "> f(x_i) = e^xi / (sum(e^x_j))\n",
        "> ```\n",
        ">\n",
        "Neural networks can take on many different architectures, but one of the most common is the feedforward network. In a feedforward network, inputs are fed through a series of hidden layers before producing an output. Each layer contains multiple neurons, and the connections between neurons are weighted according to their importance.\n",
        "\n",
        "Text-based diagrams can be used to visualize the structure of a neural network. For example, a simple feedforward network with three input nodes, two hidden layers of four neurons each, and one output node could be represented as:\n",
        "\n",
        "```\n",
        "[ Input Layer ]\n",
        "   |  |  |\n",
        "   v  v  v\n",
        "[ Hidden Layer 1 ]\n",
        "   |  |  |\n",
        "   v  v  v\n",
        "[ Hidden Layer 2 ]\n",
        "   |  |  |\n",
        "   v  v  v\n",
        "[ Output Layer ]\n",
        "```\n",
        "\n",
        "Neural networks are powerful tools for modeling complex relationships in data and have proven to be incredibly effective in a wide range of applications. With an understanding of activation functions, network architectures, and other key concepts, developers can leverage neural networks to build powerful and intelligent systems."
      ],
      "metadata": {
        "id": "riEg4Tbo3vT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Main Components of TensorFlow"
      ],
      "metadata": {
        "id": "I-FnHhRZwqnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1. **Tensors:**\n",
        ">    - Tensors are the fundamental data structures in TensorFlow.\n",
        ">    - Tensors are multi-dimensional arrays that can hold data of any data type.\n",
        ">    - They are the primary means of representing and manipulating data in TensorFlow.\n",
        ">    - Tensors flow through the computational graph, carrying data between operations.\n",
        "> \n",
        "> 2. **Computational Graph:**\n",
        ">    - TensorFlow models are represented as directed acyclic graphs (DAGs) called computational graphs.\n",
        ">    - A computational graph consists of a set of nodes that represent operations and a set of edges that represent tensors flowing between the operations.\n",
        ">    - The computational graph defines the flow of data through the model and the dependencies between operations.\n",
        ">\n",
        "> 3. **Operations:**\n",
        ">    - Operations (ops) represent computations or transformations on tensors.\n",
        ">    - TensorFlow provides a wide range of operations for mathematical computations, array manipulations, control flow, and more.\n",
        ">    - Each operation takes input tensors and produces output tensors.\n",
        ">    - Operations can be simple mathematical operations (e.g., addition, multiplication) or more complex operations (e.g., matrix multiplication, convolution).\n",
        ">\n",
        "> 4. **Variables:**\n",
        ">    - Variables are special tensors that hold their values across multiple executions of the computational graph.\n",
        ">    - Variables are used to store and update model parameters (e.g., weights and biases) during training.\n",
        ">    - They have an initial value and can be modified during model optimization.\n",
        ">    - Variables are commonly used to represent the learnable parameters of a neural network.\n",
        ">\n",
        "> 5. **Placeholders:**\n",
        ">    - Placeholders are used to feed external data into TensorFlow computational graphs.\n",
        ">    - They serve as input nodes and allow you to define the expected shape and data type of the input.\n",
        ">    - Placeholders are typically used to provide training examples or mini-batches of data during model training.\n",
        ">    - The actual data is provided using a feed dictionary or a data pipeline.\n",
        ">\n",
        "> 6. **Sessions:**\n",
        ">    - A TensorFlow session encapsulates the control and state of the TensorFlow runtime.\n",
        ">    - To execute a computational graph, you create a session, initialize variables, and then run operations within the session.\n",
        ">    - Sessions allocate resources (e.g., GPU memory) and manage the execution of operations.\n",
        ">\n",
        "> 7. **Optimizers:**\n",
        ">    - TensorFlow provides various optimization algorithms to train machine learning models.\n",
        ">    - Optimizers update the values of variables based on the gradients of the loss function with respect to the variables.\n",
        ">    - Some commonly used optimizers in TensorFlow include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad.\n",
        ">\n",
        "> 8. **Saved Models:**\n",
        ">    - TensorFlow allows models to be saved and loaded for later use or deployment.\n",
        ">    - Saved models include both the model architecture (computational graph) and the learned weights.\n",
        ">    - Saved models can be used for inference, transfer learning, or deployment in production environments.\n"
      ],
      "metadata": {
        "id": "g8AQOhlQwuiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Tensors"
      ],
      "metadata": {
        "id": "gaap3rjh0IJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tensor is a fundamental **data structure** used to represent and manipulate data. It is an n-dimensional array or a generalization of a matrix that can hold data of any data type (e.g., integers, floats, strings). Tensors are the primary means of **representing data in TensorFlow, and they flow through the computational graph**, carrying data between operations.\n",
        "\n",
        "> *Tensor is a multidimensional array that is passed as input to the models.In the computation graph, only tensors are passed between operations*\n",
        "\n"
      ],
      "metadata": {
        "id": "17sNUus30Ljd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tensor is described by three parameters\n",
        "\n",
        "1.   Rank\n",
        "2.   Shape\n",
        "3.   Type\n",
        "\n"
      ],
      "metadata": {
        "id": "bSwjlODB0IPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Rank"
      ],
      "metadata": {
        "id": "JwEQejvtKFhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In TensorFlow, tensors are the fundamental data structures used for computations. The rank of a tensor refers to the number of dimensions or axes it has. It indicates the level of complexity and the number of indices required to access the elements within the tensor.\n",
        "\n",
        "TensorFlow supports tensors of various ranks, from scalars (rank-0) to n-dimensional tensors (rank-n). \n",
        "\n",
        "\n",
        "\n",
        "> 1. **Rank 0 (Scalars):**\n",
        ">    - A rank-0 tensor, also known as a scalar, represents a single number.\n",
        ">    - Scalars in TensorFlow can be created using basic data types like integers or floating-point numbers.\n",
        ">\n",
        "> 2. **Rank 1 (Vectors):**\n",
        ">    - A rank-1 tensor, also known as a vector, represents an ordered list of numbers.\n",
        ">    - Vectors can be seen as a one-dimensional array or a sequence of elements.\n",
        ">    - **Example :**  rank-1 tensors include a list of temperatures for a week or the RGB values of an image pixel.\n",
        ">\n",
        "> 3. **Rank 2 (Matrices):**\n",
        ">    - A rank-2 tensor, also known as a matrix, represents a 2-dimensional grid of numbers.\n",
        ">    - Matrices have rows and columns and are useful for various applications, such as linear algebra operations.\n",
        ">    - **Example :**  rank-2 tensors include a grayscale image or a dataset with rows and features.\n",
        ">\n",
        "> 4. **Rank 3 (3-D Tensors):**\n",
        ">    - A rank-3 tensor represents a 3-dimensional array of numbers.\n",
        ">    - It can be visualized as a collection of matrices stacked together, forming a cube-like structure.\n",
        ">    - Rank-3 tensors are commonly used in computer vision tasks, such as storing RGB images or representing volumes of data.\n",
        ">\n",
        "> 5. **Rank n (n-Dimensional Tensors):**\n",
        ">    - TensorFlow supports tensors of arbitrary ranks, representing n-dimensional arrays of numbers.\n",
        ">    - These tensors can have more than three dimensions and are used in various domains like computer vision, natural language processing, and machine learning.\n",
        "\n",
        "It's important to note that the rank of a tensor determines the number of dimensions it has, which in turn affects the shape and size of the tensor. The shape of a tensor specifies the length or size along each axis.\n",
        "\n",
        "To determine the rank of a tensor in TensorFlow, you can use the `tf.rank()` function, which returns a scalar tensor representing the rank.\n"
      ],
      "metadata": {
        "id": "wzG_WNO7KRY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Shape\n",
        "\n"
      ],
      "metadata": {
        "id": "4RLxEZ3jLISe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow tensor shape represents the dimensions or sizes of each axis in a tensor. It provides information about the structure and organization of the data stored in the tensor.\n",
        "\n",
        "> 1. **Scalar Shape:**\n",
        ">    - A scalar tensor has an empty shape since it represents a single value.\n",
        ">    - Example: `()` or a scalar value like `5`.\n",
        "> \n",
        "> 2. **Rank 1 Shape (Vector Shape):**\n",
        ">    - A rank-1 tensor, or a vector, has a shape defined by a single dimension representing the length of the vector.\n",
        ">    - Example: `(5,)` or a vector `[1, 2, 3, 4, 5]`.\n",
        ">\n",
        "> 3. **Rank 2 Shape (Matrix Shape):**\n",
        ">    - A rank-2 tensor, or a matrix, has a shape defined by two dimensions: the number of rows and the number of columns.\n",
        ">    - Example: `(3, 4)` or a matrix represented as:\n",
        ">     ```python\n",
        ">     [[1, 2, 3, 4],\n",
        ">      [5, 6, 7, 8],\n",
        ">      [9, 10, 11, 12]]\n",
        ">     ```\n",
        ">\n",
        "> 4. **Rank 3 Shape (3D Tensor Shape):**\n",
        ">    - A rank-3 tensor has a shape defined by three dimensions: length, width, and depth.\n",
        ">    - Example: `(32, 32, 3)` or an RGB image with dimensions 32x32 and 3 color channels.\n",
        ">\n",
        "> 5. **Rank n Shape (n-Dimensional Tensor Shape):**\n",
        ">    - A rank-n tensor represents an n-dimensional array of values.\n",
        ">    - Example: `(2, 2, 2, 2)` or a 4D tensor represented as:\n",
        ">      ```python\n",
        ">      [[[1, 2],\n",
        ">        [3, 4]],\n",
        ">      \n",
        ">       [[5, 6],\n",
        ">        [7, 8]]]\n",
        ">      ```\n",
        "\n",
        "`tf.shape(tensor)` can be used to get the shape of the tensor.\n"
      ],
      "metadata": {
        "id": "kPEHxqMOQ6CL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 Data Type"
      ],
      "metadata": {
        "id": "EcR9CRZrR3Aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| Datatype      | Description                                      | Limits                                                |\n",
        "|---------------|--------------------------------------------------|-------------------------------------------------------|\n",
        "| `tf.float16`  | 16-bit floating-point                             | Range: -65504 to 65504                                |\n",
        "| `tf.float32`  | 32-bit floating-point (standard for most models)  | Range: -3.4028235e38 to 3.4028235e38                  |\n",
        "| `tf.float64`  | 64-bit floating-point                             | Range: -1.7976931348623157e308 to 1.7976931348623157e308 |\n",
        "| `tf.int8`     | 8-bit integer                                     | Range: -128 to 127                                    |\n",
        "| `tf.int16`    | 16-bit integer                                    | Range: -32768 to 32767                                |\n",
        "| `tf.int32`    | 32-bit integer                                    | Range: -2147483648 to 2147483647                      |\n",
        "| `tf.int64`    | 64-bit integer                                    | Range: -9223372036854775808 to 9223372036854775807    |\n",
        "| `tf.uint8`    | 8-bit unsigned integer                            | Range: 0 to 255                                       |\n",
        "| `tf.bool`     | Boolean (True/False)                              | True or False                                         |\n",
        "\n"
      ],
      "metadata": {
        "id": "VIhK-6MrSai4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.4  Types of Tensors"
      ],
      "metadata": {
        "id": "7J65016J4nx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1. **Constant Tensors:**\n",
        "> Constant tensors have a fixed value and cannot be changed during execution. They are often used to represent fixed values or parameters in a model.\n",
        ">"
      ],
      "metadata": {
        "id": "2GRouVUB5D8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant([1, 2, 3])\n",
        "print(a)  # Output: Tensor(\"Const:0\", shape=(3,), dtype=int32)"
      ],
      "metadata": {
        "id": "pT6dVPRT5Gb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 2. **Variable Tensors:**\n",
        ">Variable tensors are mutable tensors that can be changed during execution. They are commonly used to represent model parameters that need to be optimized during training. (Only Mutable type)\n"
      ],
      "metadata": {
        "id": "AVyCRbsM5Rda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a variable tensor\n",
        "b = tf.Variable([4, 5, 6])\n",
        "print(b)  \n",
        "# Output: <tf.Variable 'Variable:0' shape=(3,) dtype=int32_ref>"
      ],
      "metadata": {
        "id": "AtVDrer25Sj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 3. **Placeholder Tensors:**\n",
        "> Placeholder tensors act as empty slots that are filled with actual data during the execution of a TensorFlow graph. They are typically used to feed input data into the model."
      ],
      "metadata": {
        "id": "ZlzmgN6R5bPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a placeholder tensor\n",
        "input_data = tf.placeholder(tf.float32, shape=(None, 10))\n",
        "print(input_data)  \n",
        "# Output: Tensor(\"Placeholder:0\", shape=(?, 10), dtype=float32)"
      ],
      "metadata": {
        "id": "1emfeJyG5c4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 4. **Sparse Tensors:**\n",
        "> Sparse tensors are used to efficiently represent tensors with a large number of elements that are mostly zero. They store only the non-zero values and their indices, reducing memory usage."
      ],
      "metadata": {
        "id": "Gc0Aea8E5dXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sparse tensor\n",
        "indices = tf.constant([[0, 0], [1, 2]])\n",
        "values = tf.constant([1, 2], dtype=tf.float32)\n",
        "shape = tf.constant([3, 4])\n",
        "sparse_tensor = tf.sparse.SparseTensor(indices, values, shape)\n",
        "print(sparse_tensor)  \n",
        "# Output: SparseTensor(indices=Tensor(...), values=Tensor(...), dense_shape=Tensor(...))"
      ],
      "metadata": {
        "id": "pX3SnGaq5df-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 5. **Ragged Tensors:**\n",
        "> Ragged tensors are used to represent sequences with varying lengths. They are useful when dealing with data that doesn't have a fixed shape, such as sentences of different lengths."
      ],
      "metadata": {
        "id": "j-8y8buZ5dmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ragged tensor\n",
        "ragged_tensor = tf.ragged.constant([[1, 2, 3], [4, 5], [6]])\n",
        "print(ragged_tensor)  \n",
        "# Output: <tf.RaggedTensor [[1, 2, 3], [4, 5], [6]]>"
      ],
      "metadata": {
        "id": "OVqTdtMU5drC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> 6. **String Tensors:**\n",
        "> String tensors are used to handle textual data. They are often used for text processing tasks, such as natural language processing.\n"
      ],
      "metadata": {
        "id": "HJW_N_WL5KXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string tensor\n",
        "string_tensor = tf.constant(\"Hello, TensorFlow!\")\n",
        "print(string_tensor)  \n",
        "# Output: Tensor(\"Const:0\", shape=(), dtype=string)"
      ],
      "metadata": {
        "id": "G3jICyup5-Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Computational Graphs"
      ],
      "metadata": {
        "id": "Efux7UCVw3eO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computational graphs play a crucial role in TensorFlow.TensorFlow represents computations as a directed graph, where nodes represent mathematical operations, and edges represent the flow of data between those operations. \n",
        "\n",
        "\n",
        "> 1. **Nodes and Operations:**\n",
        "> In TensorFlow, each node in a computational graph represents an operation. Operations can be as simple as basic mathematical operations like addition or multiplication, or they can be complex functions like convolution or matrix multiplication. Each operation takes one or more tensors as input and produces one or more tensors as output.\n",
        ">\n",
        ">\n",
        "> 2. **Graph Construction:**\n",
        "> To build a computational graph in TensorFlow, you first define the operations you want to perform and the tensors involved. TensorFlow provides a high-level API called Keras, which simplifies the process of building and training deep learning models. Alternatively, you can use the lower-level TensorFlow API to define operations and construct the graph manually.\n",
        ">\n",
        "> 3. **Graph Execution:**\n",
        "> Once the computational graph is constructed, can execute it by running a TensorFlow session. A session encapsulates the environment in which the graph operations are executed and provides methods to feed input data and retrieve output data. During execution, TensorFlow automatically determines the dependencies between operations and optimizes the computation for efficiency.\n",
        ">\n",
        "> 4. **Lazy Evaluation:**\n",
        "> TensorFlow uses lazy evaluation, which means that the computations are not immediately executed when the operations are defined. Instead, TensorFlow waits until it is explicitly instructed to run the graph within a session. This allows TensorFlow to optimize the graph and perform various optimizations, such as common subexpression elimination and constant folding, before executing the operations.\n",
        ">\n",
        "> 5. **Automatic Differentiation:**\n",
        "> One of the key features of TensorFlow is its ability to automatically compute gradients for training models using backpropagation. TensorFlow keeps track of the operations that are executed during forward computation and constructs an additional backward graph that computes the gradients. This enables efficient gradient computation for training deep learning models.\n",
        ">\n",
        "> 6. **Distributed Computing:**\n",
        "> TensorFlow supports distributed computing, allowing you to train and execute models across multiple devices or machines. Computational graphs can be divided into smaller subgraphs and executed in parallel, enabling efficient distributed training and inference. TensorFlow provides tools and APIs for managing distributed training and synchronization between devices.\n",
        ">\n",
        "> 7. V**isualization and Debugging:**\n",
        "> TensorFlow provides various tools for visualizing and debugging computational graphs. The TensorBoard tool allows you to visualize the graph structure, monitor training progress, and analyze model performance. Additionally, TensorFlow provides built-in debugging capabilities, such as adding breakpoints and inspecting intermediate tensor values during graph execution.\n",
        "\n",
        "\n",
        "In TensorFlow 2.0, the eager execution mode was introduced. Eager execution enables immediate execution of operations, eliminating the need for sessions and graph construction. However, computational graphs are still used under the hood for performance optimization and automatic differentiation."
      ],
      "metadata": {
        "id": "CO3TreYSzBwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Tensor Handling"
      ],
      "metadata": {
        "id": "QaqokRSEdfap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Creating Tensors"
      ],
      "metadata": {
        "id": "rkC6OvutdseZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Creating Tensors in TensorFlow:**\n",
        "\n",
        "   a. Using `tf.constant()`:\n",
        "   - `tf.constant(value, dtype=None, shape=None, name='Const')` creates a constant tensor with a specified value, datatype, and shape.\n",
        "   - Example:\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "\n",
        "     # Creating a 1D tensor with values [1, 2, 3]\n",
        "     tensor = tf.constant([1, 2, 3], dtype=tf.float32)\n",
        "     print(tensor)  # Output: Tensor(\"Const:0\", shape=(3,), dtype=float32)\n",
        "     ```\n",
        "\n",
        "   b. Using `tf.Variable()`:\n",
        "   - `tf.Variable(initial_value, dtype=None, name=None)` creates a mutable tensor with an initial value, datatype, and an optional name.\n",
        "   - Example:\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "\n",
        "     # Creating a variable tensor with initial value 5\n",
        "     tensor = tf.Variable(5, dtype=tf.int32)\n",
        "     print(tensor)  # Output: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>\n",
        "     ```\n",
        "\n",
        "2. **Creating Tensors in NumPy:**\n",
        "\n",
        "   a. Using `numpy.array()`:\n",
        "   - `numpy.array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)` creates an array from a specified object with a desired datatype.\n",
        "   - Example:\n",
        "     ```python\n",
        "     import numpy as np\n",
        "\n",
        "     # Creating a 1D NumPy array with values [1, 2, 3]\n",
        "     array = np.array([1, 2, 3], dtype=np.float32)\n",
        "     print(array)  # Output: [1. 2. 3.]\n",
        "     ```\n",
        "\n",
        "   b. Using specialized methods:\n",
        "   - NumPy provides various methods like `numpy.zeros()`, `numpy.ones()`, and `numpy.random.rand()` to create tensors with specific values and shapes.\n",
        "   - Example:\n",
        "     ```python\n",
        "     import numpy as np\n",
        "\n",
        "     # Creating a tensor filled with zeros of shape (2, 3)\n",
        "     zeros_tensor = np.zeros((2, 3))\n",
        "     print(zeros_tensor)  # Output: [[0. 0. 0.]\n",
        "                          #          [0. 0. 0.]]\n",
        "\n",
        "     # Creating a tensor filled with random values of shape (3, 3)\n",
        "     random_tensor = np.random.rand(3, 3)\n",
        "     print(random_tensor)  # Output: Randomly generated 3x3 array\n",
        "     ```\n",
        "\n"
      ],
      "metadata": {
        "id": "VIMcsWZDdyl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Tensor Manipulation"
      ],
      "metadata": {
        "id": "78rVakmeeTSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 In TensorFlow"
      ],
      "metadata": {
        "id": "i264aXSNeccO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor manipulations in TensorFlow involve various operations to reshape, slice, concatenate, and perform element-wise computations on tensors. \n",
        "\n",
        "1. Reshaping Tensors:\n",
        "   - `tf.reshape()`: Reshapes a tensor into a new shape while preserving the total number of elements.\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "\n",
        "     # Reshape a tensor\n",
        "     tensor = tf.constant([[1, 2], [3, 4]])\n",
        "     reshaped_tensor = tf.reshape(tensor, (4,))\n",
        "     print(reshaped_tensor)\n",
        "     # Output: [1, 2, 3, 4]\n",
        "     ```\n",
        "\n",
        "2. Slicing Tensors:\n",
        "   - Slicing allows you to extract a portion of a tensor.\n",
        "   - TensorFlow follows similar syntax to Python's slice notation.\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "\n",
        "     # Slice a tensor\n",
        "     tensor = tf.constant([1, 2, 3, 4, 5])\n",
        "     sliced_tensor = tensor[1:4]\n",
        "     print(sliced_tensor)\n",
        "     # Output: [2, 3, 4]\n",
        "     ```\n",
        "\n",
        "3. Concatenating Tensors:\n",
        "   - `tf.concat()`: Concatenates tensors along a specific axis.\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "\n",
        "     # Concatenate tensors\n",
        "     tensor1 = tf.constant([1, 2, 3])\n",
        "     tensor2 = tf.constant([4, 5, 6])\n",
        "     concatenated_tensor = tf.concat([tensor1, tensor2], axis=0)\n",
        "     print(concatenated_tensor)\n",
        "     # Output: [1, 2, 3, 4, 5, 6]\n",
        "     ```\n",
        "\n",
        "4. Element-wise Computations:\n",
        "   - TensorFlow provides various mathematical operations to perform element-wise computations on tensors, such as addition, subtraction, multiplication, division, etc.\n",
        "   - Example:\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "\n",
        "     # Element-wise computations\n",
        "     tensor1 = tf.constant([1, 2, 3])\n",
        "     tensor2 = tf.constant([4, 5, 6])\n",
        "     sum_tensor = tensor1 + tensor2\n",
        "     print(sum_tensor)\n",
        "     # Output: [5, 7, 9]\n",
        "\n",
        "     product_tensor = tensor1 * tensor2\n",
        "     print(product_tensor)\n",
        "     # Output: [4, 10, 18]\n",
        "     ```\n",
        "\n"
      ],
      "metadata": {
        "id": "BQzfPGEAetLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 Numpy"
      ],
      "metadata": {
        "id": "ve_93foeeyFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Reshaping Tensors:\n",
        "   - You can change the shape of a tensor using the `numpy.reshape()` function.\n",
        "   - Syntax: `numpy.reshape(array, newshape)`\n",
        "   - Example:\n",
        "     ```python\n",
        "     import numpy as np\n",
        "\n",
        "     # Reshape a tensor\n",
        "     tensor = np.array([1, 2, 3, 4, 5, 6])\n",
        "     reshaped_tensor = np.reshape(tensor, (2, 3))\n",
        "     print(reshaped_tensor)\n",
        "     # Output:\n",
        "     # [[1, 2, 3],\n",
        "     #  [4, 5, 6]]\n",
        "     ```\n",
        "\n",
        "2. Slicing Tensors:\n",
        "   - You can extract specific elements or sub-tensors from a tensor using indexing and slicing operations.\n",
        "   - Example:\n",
        "     ```python\n",
        "     import numpy as np\n",
        "\n",
        "     # Slice a tensor\n",
        "     tensor = np.array([1, 2, 3, 4, 5, 6])\n",
        "     sliced_tensor = tensor[1:4]\n",
        "     print(sliced_tensor)  # Output: [2, 3, 4]\n",
        "     ```\n",
        "\n",
        "3. Concatenating Tensors:\n",
        "   - You can combine tensors along a specified axis using the `numpy.concatenate()` function.\n",
        "   - Syntax: `numpy.concatenate((array1, array2, ...), axis=0)`\n",
        "   - Example:\n",
        "     ```python\n",
        "     import numpy as np\n",
        "\n",
        "     # Concatenate tensors\n",
        "     tensor1 = np.array([1, 2, 3])\n",
        "     tensor2 = np.array([4, 5, 6])\n",
        "     concatenated_tensor = np.concatenate((tensor1, tensor2), axis=0)\n",
        "     print(concatenated_tensor)\n",
        "     # Output: [1, 2, 3, 4, 5, 6]\n",
        "     ```\n",
        "\n",
        "4. Mathematical Operations on Tensors:\n",
        "   - NumPy supports various mathematical operations on tensors, such as element-wise operations (`np.add()`, `np.multiply()`, etc.), matrix multiplication (`np.dot()`, `np.matmul()`, etc.), reduction operations (`np.sum()`, `np.mean()`, etc.), and more.\n",
        "   - Example:\n",
        "     ```python\n",
        "     import numpy as np\n",
        "\n",
        "     # Perform mathematical operations on tensors\n",
        "     tensor1 = np.array([1, 2, 3])\n",
        "     tensor2 = np.array([4, 5, 6])\n",
        "     sum_tensor = np.add(tensor1, tensor2)\n",
        "     print(sum_tensor)  # Output: [5, 7, 9]\n",
        "     ```\n",
        "\n"
      ],
      "metadata": {
        "id": "M_0Ps7Lse2PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Curve Fitting"
      ],
      "metadata": {
        "id": "tt41_N3d4mOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 2D Model Training "
      ],
      "metadata": {
        "id": "m3-ezZc95Vw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-_amg-WG5gpA"
      }
    }
  ]
}